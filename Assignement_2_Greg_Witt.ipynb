{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b133c7d-94cd-4d88-ad3b-ec756ed6c369",
   "metadata": {},
   "source": [
    "# Assignment 2: Large Language Models for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088e649-206e-4c4f-a802-ef860fc873f1",
   "metadata": {},
   "source": [
    "### CS 410/510 Large Language Models Fall 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995529cd-a516-45b5-ab5c-a76d668871d8",
   "metadata": {},
   "source": [
    "#### Greg Witt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b870bd-7be3-4447-b531-77dbaafec5f1",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eaa395-0356-4d84-bd75-7c632ede4141",
   "metadata": {},
   "source": [
    "**Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "101775ae-ee6b-419e-ba63-291fe04b99c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "85077.27s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.0.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dfd64a-475d-4daf-8546-7ebcd287e6e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6dc5a88f-c340-4bb8-a21c-d0c68f57ac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual training dataset: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1839\n",
      "}) \n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    Joe Biden on economy: 'Something is wrong': Vice President Joe Biden warned on Monday that \"something is wrong\" with the American eco... \n",
      "\n",
      "    Label: \n",
      "    0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# load the training set of tweets\n",
    "ds_train = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"english\", split=\"train\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "print(f\"\"\"tweet_sentiment_multilingual training dataset: \n",
    "    -----------------------------------------\n",
    "        {ds_train} \n",
    "    \"\"\")\n",
    "random_tweet_index = random.randint(0,1839)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_train['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_train['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "50bf010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_sentiment_multilingual validation set: \n",
      "    -----------------------------------------\n",
      "        Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 324\n",
      "})\n",
      "    \n",
      "\n",
      "    Random Tweet:\n",
      "    \"The BAGRANGI new Pic,Of SALMAN khan That VERY FAMOUS IN PAK CENEMA'S at the 1st day of EID that pic,made 1.5 milion Rs  Lolywood/Bolywood\" \n",
      "\n",
      "    Label: \n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the training set of tweets\n",
    "ds_validation = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"english\", split=\"validation\")\n",
    "# the dataset labels {0 : negative, 1: neutral, 2: positive }\n",
    "print(f\"\"\"tweet_sentiment_multilingual validation set: \n",
    "    -----------------------------------------\n",
    "        {ds_validation}\n",
    "    \"\"\")\n",
    "random_tweet_index = random.randint(0,324)\n",
    "print(f\"\"\"\n",
    "    Random Tweet:\n",
    "    {ds_validation['text'][random_tweet_index]}\n",
    "\n",
    "    Label: \n",
    "    {ds_validation['label'][random_tweet_index] }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65fbc96",
   "metadata": {},
   "source": [
    "## Load the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310abc7",
   "metadata": {},
   "source": [
    "**Load Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "69fc74e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "85086.12s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.46.1)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.venv/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.venv/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebbb03f",
   "metadata": {},
   "source": [
    "#### [Lama 3.2 1B](https://huggingface.co/meta-llama/Llama-3.2-1B) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c0c87e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "llama_3_2_1B = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "llama_3_2_1B_tokenizer = AutoTokenizer.from_pretrained(llama_3_2_1B)\n",
    "\n",
    "llama_3_2_1B_model = AutoModelForCausalLM.from_pretrained(llama_3_2_1B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4b6e02d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "classifier = pipeline(model=llama_3_2_1B, task=\"text-classification\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b02fd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer assigns the probability \n",
    "\"Text: I hate it! \\nSentiment (positive, negative, neutral):\"\n",
    "\n",
    "prompt_approach_two = f\"\"\"{tweet} Sentiment: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c076a312",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour text to classify here.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Get model predictions\u001b[39;00m\n\u001b[1;32m      8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Text to classify\n",
    "text = \"Your text to classify here.\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get model predictions\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Determine the predicted class\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae1260ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mExperiment\u001b[49m \u001b[38;5;66;03m#2 \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Experiment' is not defined"
     ]
    }
   ],
   "source": [
    "Experiment #2 \n",
    "\n",
    "1 prompt  2 positive , 2 negative, 2 neutral k = 2 {tweet} : Sentiment {?} =>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "363c01b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'else' after 'if' expression (2972846960.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    google_this if needed.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected 'else' after 'if' expression\n"
     ]
    }
   ],
   "source": [
    "# 3 \n",
    "google_this if needed.\n",
    "chain_of_thought = \"think about this step by step this sound positive, negative, neurtal?\\n\"\n",
    "f\"{chain_of_thought} {tweet}\"\n",
    "\n",
    "# emotion based: pull on compassion strings.\n",
    "\"I need this for my job: classify as negative, positive or neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244de673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ab645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1eccdae2",
   "metadata": {},
   "source": [
    "#### [Phi 3.5 Instruct]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7003448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a01870a9",
   "metadata": {},
   "source": [
    "**Load The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ff264a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a50af457bc64d7c9f303b251e7e2e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m phi_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/Phi-3.5-mini-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m phi_3_5_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(phi_model)\n\u001b[0;32m----> 9\u001b[0m phi_3_5_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphi_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4225\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4216\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4218\u001b[0m     (\n\u001b[1;32m   4219\u001b[0m         model,\n\u001b[1;32m   4220\u001b[0m         missing_keys,\n\u001b[1;32m   4221\u001b[0m         unexpected_keys,\n\u001b[1;32m   4222\u001b[0m         mismatched_keys,\n\u001b[1;32m   4223\u001b[0m         offload_index,\n\u001b[1;32m   4224\u001b[0m         error_msgs,\n\u001b[0;32m-> 4225\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4232\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4236\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4237\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4245\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4246\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4751\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assign_to_params_buffers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4748\u001b[0m         assign_to_params_buffers \u001b[38;5;241m=\u001b[39m check_support_param_buffer_assignment(\n\u001b[1;32m   4749\u001b[0m             model_to_load, state_dict, start_prefix\n\u001b[1;32m   4750\u001b[0m         )\n\u001b[0;32m-> 4751\u001b[0m     error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\n\u001b[1;32m   4753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;66;03m# force memory release\u001b[39;00m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:775\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, assign_to_params_buffers)\n\u001b[0;32m--> 775\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:773\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 773\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:773\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 773\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 773 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:773\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 773\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:769\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    767\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2441\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2439\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2440\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2441\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2443\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswapping\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopying\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# model name\n",
    "phi_model = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "phi_3_5_tokenizer = AutoTokenizer.from_pretrained(phi_model)\n",
    "\n",
    "phi_3_5_model = AutoModelForCausalLM.from_pretrained(phi_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518e687",
   "metadata": {},
   "source": [
    "## Experiments 1: Zero-Shot Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562780e-68e0-43ff-a06a-72ce676a2f30",
   "metadata": {},
   "source": [
    "### Create Prompt Iteration Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "973c79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\\\"{}\\\" \\n Sentiment (positive (2), negative (0), neutral (1)): \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d681b",
   "metadata": {},
   "source": [
    "### Llama 3.3 1B: Zero Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed8764fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31587\n",
      "43324\n",
      "60668\n"
     ]
    }
   ],
   "source": [
    "# encode possible labels \n",
    "positive_id = llama_3_2_1B_tokenizer.encode(\"positive\", add_special_tokens=False)[0]\n",
    "negative_id = llama_3_2_1B_tokenizer.encode(\"negative\", add_special_tokens=False)[0]\n",
    "neutral_id = llama_3_2_1B_tokenizer.encode(\"neutral\", add_special_tokens=False)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf806a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6aaae4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sanitize_tweet(tweet):\n",
    "    sanitized_tweet = re.sub(r\"(\\@\\w+ | \\#\\w+)\",\"\", tweet)\n",
    "    return sanitized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "360ce730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think after Charlie Hebdo the French did NOT react as the US did after 9/11. But they may do this time around. \n",
      "Beautiful Bouquet with our Beautiful Bentley  ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sanitize tweets\n",
    "# removes user tags\n",
    "tweet_user = \"@user @user I think after Charlie Hebdo the French did NOT react as the US did after 9/11. But they may do this time around. \"\n",
    "\n",
    "user_swap = re.sub(r'\\@\\w+',\"\", tweet_user)\n",
    "\n",
    "sanitize_tweet(tweet_user)\n",
    "\n",
    "# hash tag heavy tweet\n",
    "hash_tag_tweet = \"Beautiful Bouquet with our Beautiful Bentley #bride #groom #wedding #wednesday #weddingcars #love   #Repost...\"\n",
    "\n",
    "tag_swap = re.sub(r'\\#\\w+',\"\", hash_tag_tweet)\n",
    "\n",
    "sanitize_tweet(hash_tag_tweet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "61a057d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregwitt/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_scores` is. When `return_dict_in_generate` is not `True`, `output_scores` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: \"@user @user I think after Charlie Hebdo the French did NOT react as the US did after 9/11. But they may do this time around. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): 0\n",
      "Ground Truth: 0\n",
      "Generated Response: \"\"Interview with Devon Alexander \\\"\"\"\"Speed Kills\\\"\"\"\" (VIDEO)  On Tuesday Oct 16th we had the privilege of catch up with... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): 0\n",
      "Ground Truth: 1\n",
      "Generated Response: \"Hold on... Sam Smith may do the theme to Spectre!? Dope!!!!!! #007 #SPECTRE #JamesBond \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): 2\n",
      "Ground Truth: 2\n",
      "Generated Response: \"kingpin Saudi Arabia posted a record $98 billion budget deficit in 2015 due to the sharp fall in oil prices finance ministry said on Monday \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): 0\n",
      "Ground Truth: 0\n",
      "Generated Response: \"Gonna watch Final Destination 5 tonight. I always leave the theater so afraid of everything. No huge escalators for sure :S \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): 1\n",
      "Ground Truth: 1\n",
      "Generated Response: \"Beautiful Bouquet with our Beautiful Bentley #bride #groom #wedding #wednesday #weddingcars #love   #Repost... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): 1\n",
      "Ground Truth: 2\n",
      "Generated Response: \"@user @user Islam is an Abrahamic faith, Andrew. It may make you feel a little uneasy but it's the same God you worship. Sorry.\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): 2\n",
      "Ground Truth: 0\n",
      "Generated Response: \"Harper's Worst Offense against Refugees may be Climate Record as rising temperatures add to chaos in the Middle East \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): 2\n",
      "Ground Truth: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# generate the response\u001b[39;00m\n\u001b[1;32m      9\u001b[0m prompt_ids \u001b[38;5;241m=\u001b[39m llama_3_2_1B_tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllama_3_2_1B_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_3_2_1B_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# get the response tokens from the model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# iterate through the validation set\n",
    "for tweet, label in zip(ds_validation['text'], ds_validation['label']):\n",
    "    prompt_tweet = tweet\n",
    "    gt_label = label\n",
    "    # combine the prompt\n",
    "    prompt = prompt_template.format(prompt_tweet)\n",
    "    \n",
    "    # generate the response\n",
    "    prompt_ids = llama_3_2_1B_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = llama_3_2_1B_model.generate(\n",
    "                        prompt_ids,\n",
    "                        pad_token_id=llama_3_2_1B_tokenizer.eos_token_id,\n",
    "                        max_new_tokens=1,\n",
    "                        output_scores=True\n",
    "                    )\n",
    "    \n",
    "    # get the response tokens from the model\n",
    "    generated_tokens = outputs[-1]\n",
    "    \n",
    "    generated_response = llama_3_2_1B_tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Generated Response: {generated_response}\")\n",
    "    print(f\"Ground Truth: {gt_label}\")\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     outputs = llama_3_2_1B_model(**prompt_ids)\n",
    "    #     logits = outputs.logits\n",
    "\n",
    "    # predicted_class_id = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # predicted_class_label = model.config.id2label[predicted_class_id.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "23fa5748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31587, 43324, 60668]\n"
     ]
    }
   ],
   "source": [
    "sentiment_words = ['positive','negative','neutral']\n",
    "sentiment_ids = [ids[0] for ids in llama_3_2_1B_tokenizer(sentiment_words, add_special_tokens=False).input_ids]\n",
    "print(sentiment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "af668c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 31587, 'negative': 43324, 'neutral': 60668}\n"
     ]
    }
   ],
   "source": [
    "sentiment_ids = {}\n",
    "# encode possible labels \n",
    "sentiment_ids[\"positive\"] = llama_3_2_1B_tokenizer.encode(\"positive\", add_special_tokens=False)[0]\n",
    "sentiment_ids[\"negative\"] = llama_3_2_1B_tokenizer.encode(\"negative\", add_special_tokens=False)[0]\n",
    "sentiment_ids[\"neutral\"] = llama_3_2_1B_tokenizer.encode(\"neutral\", add_special_tokens=False)[0]\n",
    "\n",
    "print(sentiment_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "44be457a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive\n",
      "Sentiment_ID: 31587\n",
      "Sentiment: negative\n",
      "Sentiment_ID: 43324\n",
      "Sentiment: neutral\n",
      "Sentiment_ID: 60668\n"
     ]
    }
   ],
   "source": [
    "for sentiment in sentiment_ids:\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(f\"Sentiment_ID: {sentiment_ids[sentiment]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f44956bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt_tweet: I think after Charlie Hebdo the French did NOT react as the US did after 9/11. But they may do this time around. \n",
      "{'positive': -9.874181747436523, 'negative': -11.539008140563965, 'neutral': -11.533194541931152}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"I think after Charlie Hebdo the French did NOT react as the US did after 9/11. But they may do this time around. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: \"Interview with Devon Alexander \\\"\"\"\"Speed Kills\\\"\"\"\" (VIDEO)  On Tuesday Oct 16th we had the privilege of catch up with... \n",
      "{'positive': -9.161550521850586, 'negative': -11.263745307922363, 'neutral': -10.887892723083496}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"\"Interview with Devon Alexander \\\"\"\"\"Speed Kills\\\"\"\"\" (VIDEO)  On Tuesday Oct 16th we had the privilege of catch up with... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: Hold on... Sam Smith may do the theme to Spectre!? Dope!!!!!! \n",
      "{'positive': -9.814620018005371, 'negative': -11.960851669311523, 'neutral': -11.809019088745117}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"Hold on... Sam Smith may do the theme to Spectre!? Dope!!!!!! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: kingpin Saudi Arabia posted a record $98 billion budget deficit in 2015 due to the sharp fall in oil prices finance ministry said on Monday \n",
      "{'positive': -10.686995506286621, 'negative': -12.42932415008545, 'neutral': -11.51211166381836}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"kingpin Saudi Arabia posted a record $98 billion budget deficit in 2015 due to the sharp fall in oil prices finance ministry said on Monday \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: Gonna watch Final Destination 5 tonight. I always leave the theater so afraid of everything. No huge escalators for sure :S \n",
      "{'positive': -10.941680908203125, 'negative': -12.832448959350586, 'neutral': -12.955827713012695}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"Gonna watch Final Destination 5 tonight. I always leave the theater so afraid of everything. No huge escalators for sure :S \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: Beautiful Bouquet with our Beautiful Bentley  ... \n",
      "{'positive': -8.952688217163086, 'negative': -11.1389799118042, 'neutral': -10.434118270874023}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"Beautiful Bouquet with our Beautiful Bentley  ... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: Islam is an Abrahamic faith, Andrew. It may make you feel a little uneasy but it's the same God you worship. Sorry.\" \n",
      "{'positive': -9.628457069396973, 'negative': -11.983563423156738, 'neutral': -11.792579650878906}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"Islam is an Abrahamic faith, Andrew. It may make you feel a little uneasy but it's the same God you worship. Sorry.\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: Harper's Worst Offense against Refugees may be Climate Record as rising temperatures add to chaos in the Middle East \n",
      "{'positive': -11.200645446777344, 'negative': -12.547121047973633, 'neutral': -13.033374786376953}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"Harper's Worst Offense against Refugees may be Climate Record as rising temperatures add to chaos in the Middle East \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: call Hafiz saeed sir he may help u out. Maybe Pope can b handy . Try it. \n",
      "{'positive': -9.027673721313477, 'negative': -10.95682430267334, 'neutral': -11.421697616577148}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"call Hafiz saeed sir he may help u out. Maybe Pope can b handy . Try it. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: Disappointed the Knicks vs Nets game got canceled tonight\\u002c but I\\u2019m even more hyped for Knicks vs Heat on Friday! \n",
      "{'positive': -11.655611038208008, 'negative': -12.956754684448242, 'neutral': -13.735713958740234}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"Disappointed the Knicks vs Nets game got canceled tonight\\u002c but I\\u2019m even more hyped for Knicks vs Heat on Friday! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: \"LONDON (AP) \"\" Prince George celebrates his second birthday on Wednesday and while he's just a toddler, he's al...  \n",
      "{'positive': -9.640953063964844, 'negative': -11.291556358337402, 'neutral': -10.707857131958008}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"\"LONDON (AP) \"\" Prince George celebrates his second birthday on Wednesday and while he's just a toddler, he's al...  \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: \"\\\"\"\"\"@_eryflores: March 16 Luke Bryan is gonna at the Houston Rodeo. I HAVE to go\\u002c Its a MUST!\\\"\"\"\"\" \n",
      "{'positive': -9.344711303710938, 'negative': -11.380562782287598, 'neutral': -11.035704612731934}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"\"\\\"\"\"\"@_eryflores: March 16 Luke Bryan is gonna at the Houston Rodeo. I HAVE to go\\u002c Its a MUST!\\\"\"\"\"\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: It is reality that ISIS are on the march in Turkey and Erdogan can't wait to receive them with open arms \n",
      "{'positive': -11.595776557922363, 'negative': -13.202943801879883, 'neutral': -13.172237396240234}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"It is reality that ISIS are on the march in Turkey and Erdogan can't wait to receive them with open arms \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet:  A trunk show by Pipa & bella & EKSMS on Nov 1st @ Escobar with complimentary Cocktail workshop & designer Jewelry.RSVP to us \n",
      "{'positive': -10.830535888671875, 'negative': -12.426498413085938, 'neutral': -11.587882041931152}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \" A trunk show by Pipa & bella & EKSMS on Nov 1st @ Escobar with complimentary Cocktail workshop & designer Jewelry.RSVP to us \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: PM ready for reply on coal blocks: Congress: New Delhi\\u002c Aug 22 (IANS) With the Bharatiya Janata Party (BJP)... \n",
      "{'positive': -11.572053909301758, 'negative': -13.475152015686035, 'neutral': -12.86563491821289}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"PM ready for reply on coal blocks: Congress: New Delhi\\u002c Aug 22 (IANS) With the Bharatiya Janata Party (BJP)... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: \"More like boring eagles\"\"\"\"\"\"\"\"@Tunnyking: C'mon bro, Go out and support the Super Eagles I hate international breaks\" \n",
      "{'positive': -10.706635475158691, 'negative': -12.441640853881836, 'neutral': -13.124046325683594}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"\"More like boring eagles\"\"\"\"\"\"\"\"@Tunnyking: C'mon bro, Go out and support the Super Eagles I hate international breaks\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: \"The BAGRANGI new Pic,Of SALMAN khan That VERY FAMOUS IN PAK CENEMA'S at the 1st day of EID that pic,made 1.5 milion Rs  Lolywood/Bolywood\" \n",
      "{'positive': -9.137435913085938, 'negative': -11.223771095275879, 'neutral': -11.49982738494873}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"\"The BAGRANGI new Pic,Of SALMAN khan That VERY FAMOUS IN PAK CENEMA'S at the 1st day of EID that pic,made 1.5 milion Rs  Lolywood/Bolywood\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: This Saturday &amp; Sunday come join us the at the Pomona Fairplex! Your ticket can WIN you a Brand New Car! \n",
      "{'positive': -9.86105728149414, 'negative': -12.020946502685547, 'neutral': -11.80160140991211}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"This Saturday &amp; Sunday come join us the at the Pomona Fairplex! Your ticket can WIN you a Brand New Car! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: \"I do worry about the mentality of the average tweeter when I see Jeremy Kyle\\u002c and \\\"\"\"\"Christmas\\\"\"\"\" trending on November 1st...\" \n",
      "{'positive': -11.441130638122559, 'negative': -12.456974029541016, 'neutral': -13.474821090698242}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"\"I do worry about the mentality of the average tweeter when I see Jeremy Kyle\\u002c and \\\"\"\"\"Christmas\\\"\"\"\" trending on November 1st...\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: we want you to milan tomorrow !!! \n",
      "{'positive': -11.119251251220703, 'negative': -12.788361549377441, 'neutral': -13.306960105895996}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"we want you to milan tomorrow !!! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: Sharknado 3 may be the best film I've seen yet. \n",
      "{'positive': -12.071282386779785, 'negative': -13.73818588256836, 'neutral': -14.757564544677734}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"Sharknado 3 may be the best film I've seen yet. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: Celebrity Big Brother: Daniel's eviction stirs up bad feelings in the house: Daniel Baldwin may have left the ... \n",
      "{'positive': -10.59022045135498, 'negative': -12.6358003616333, 'neutral': -13.21710205078125}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"Celebrity Big Brother: Daniel's eviction stirs up bad feelings in the house: Daniel Baldwin may have left the ... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: Hey David Bowie Do u want to get iPh0ne 6 for FREE? U better check my bi0. Thx \n",
      "{'positive': -9.563201904296875, 'negative': -11.216692924499512, 'neutral': -11.269148826599121}\n",
      "positive\n",
      "-----------------------------\n",
      "Tweet Prompt: \"Hey David Bowie Do u want to get iPh0ne 6 for FREE? U better check my bi0. Thx \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "Predicted Sentiment: positive\n",
      "-----------------------------\n",
      "Prompt_tweet: I think that was faster win then the rousey fight on Saturday Night! \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m prompt_ids \u001b[38;5;241m=\u001b[39m llama_3_2_1B_tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllama_3_2_1B_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# get the output logits\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:676\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:614\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    612\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 614\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# # iterate through the validation set\n",
    "for tweet, label in zip(ds_validation['text'], ds_validation['label']):\n",
    "    prompt_tweet = sanitize_tweet(tweet=tweet)\n",
    "    print(f\"Prompt_tweet: {prompt_tweet}\")\n",
    "    gt_label = label\n",
    "    # combine the prompt\n",
    "    prompt = prompt_template.format(prompt_tweet)\n",
    "    \n",
    "    prompt_ids = llama_3_2_1B_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llama_3_2_1B_model(prompt_ids)\n",
    "    \n",
    "        # get the output logits\n",
    "        logits = outputs.logits\n",
    "\n",
    "    \n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # build a list of the sentiment options and the probability per sentiment\n",
    "    tweet_sentiment_probabilities = {}\n",
    "\n",
    "    for sentiment in sentiment_ids:\n",
    "        tweet_sentiment_probabilities[sentiment] = log_probs[0, -1, sentiment_ids[sentiment]].item()\n",
    "    \n",
    "    \n",
    "    max_sentiment = max(tweet_sentiment_probabilities, key=tweet_sentiment_probabilities.get)\n",
    "    print(tweet_sentiment_probabilities)\n",
    "    print(max_sentiment)\n",
    "    \n",
    "    print(\"-----------------------------\")\n",
    "    print(f\"Tweet Prompt: {prompt}\")\n",
    "    print(f\"Ground Truth: {gt_label}\")\n",
    "    print(f\"Predicted Sentiment: {max_sentiment}\")\n",
    "    print(\"-----------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "844e8d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31587\n",
      "43324\n",
      "60668\n"
     ]
    }
   ],
   "source": [
    "sentiment_words = [\"positive\", \"negative\", \"neutral\"]\n",
    "sentiment_ids = llama_3_2_1B_tokenizer(sentiment_words, add_special_tokens=False).input_ids\n",
    "\n",
    "for sentiment in sentiment_ids:\n",
    "    print(sentiment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fdc97f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Prompt: \"@user @user I think after Charlie Hebdo the French did NOT react as the US did after 9/11. But they may do this time around. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.349617004394531, 'negative': -10.979355812072754, 'neutral': -10.924836158752441}\n",
      "Tweet Prompt: \"\"Interview with Devon Alexander \\\"\"\"\"Speed Kills\\\"\"\"\" (VIDEO)  On Tuesday Oct 16th we had the privilege of catch up with... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.161550521850586, 'negative': -11.263745307922363, 'neutral': -10.887892723083496}\n",
      "Tweet Prompt: \"Hold on... Sam Smith may do the theme to Spectre!? Dope!!!!!! #007 #SPECTRE #JamesBond \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.673638343811035, 'negative': -11.593548774719238, 'neutral': -11.763683319091797}\n",
      "Tweet Prompt: \"kingpin Saudi Arabia posted a record $98 billion budget deficit in 2015 due to the sharp fall in oil prices finance ministry said on Monday \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.686995506286621, 'negative': -12.42932415008545, 'neutral': -11.51211166381836}\n",
      "Tweet Prompt: \"Gonna watch Final Destination 5 tonight. I always leave the theater so afraid of everything. No huge escalators for sure :S \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.941680908203125, 'negative': -12.832448959350586, 'neutral': -12.955827713012695}\n",
      "Tweet Prompt: \"Beautiful Bouquet with our Beautiful Bentley #bride #groom #wedding #wednesday #weddingcars #love   #Repost... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.185047149658203, 'negative': -10.976482391357422, 'neutral': -10.538427352905273}\n",
      "Tweet Prompt: \"@user @user Islam is an Abrahamic faith, Andrew. It may make you feel a little uneasy but it's the same God you worship. Sorry.\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.06990909576416, 'negative': -11.328580856323242, 'neutral': -11.701764106750488}\n",
      "Tweet Prompt: \"Harper's Worst Offense against Refugees may be Climate Record as rising temperatures add to chaos in the Middle East \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -11.200645446777344, 'negative': -12.547121047973633, 'neutral': -13.033374786376953}\n",
      "Tweet Prompt: \"@user call Hafiz saeed sir he may help u out. Maybe Pope can b handy . Try it. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -8.83209228515625, 'negative': -10.957419395446777, 'neutral': -10.792651176452637}\n",
      "Tweet Prompt: \"Disappointed the Knicks vs Nets game got canceled tonight\\u002c but I\\u2019m even more hyped for Knicks vs Heat on Friday! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -11.655611038208008, 'negative': -12.956754684448242, 'neutral': -13.735713958740234}\n",
      "Tweet Prompt: \"\"LONDON (AP) \"\" Prince George celebrates his second birthday on Wednesday and while he's just a toddler, he's al...  \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.640953063964844, 'negative': -11.291556358337402, 'neutral': -10.707857131958008}\n",
      "Tweet Prompt: \"\"\\\"\"\"\"@_eryflores: March 16 Luke Bryan is gonna at the Houston Rodeo. I HAVE to go\\u002c Its a MUST!\\\"\"\"\"\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.344711303710938, 'negative': -11.380562782287598, 'neutral': -11.035704612731934}\n",
      "Tweet Prompt: \"It is reality that ISIS are on the march in Turkey and Erdogan can't wait to receive them with open arms \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -11.595776557922363, 'negative': -13.202943801879883, 'neutral': -13.172237396240234}\n",
      "Tweet Prompt: \"@user  A trunk show by Pipa & bella & EKSMS on Nov 1st @ Escobar with complimentary Cocktail workshop & designer Jewelry.RSVP to us \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.780923843383789, 'negative': -11.680092811584473, 'neutral': -10.716678619384766}\n",
      "Tweet Prompt: \"PM ready for reply on coal blocks: Congress: New Delhi\\u002c Aug 22 (IANS) With the Bharatiya Janata Party (BJP)... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -11.572053909301758, 'negative': -13.475152015686035, 'neutral': -12.86563491821289}\n",
      "Tweet Prompt: \"\"More like boring eagles\"\"\"\"\"\"\"\"@Tunnyking: C'mon bro, Go out and support the Super Eagles #RT @user I hate international breaks\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.810250282287598, 'negative': -12.360438346862793, 'neutral': -13.178675651550293}\n",
      "Tweet Prompt: \"\"The BAGRANGI new Pic,Of SALMAN khan That VERY FAMOUS IN PAK CENEMA'S at the 1st day of EID that pic,made 1.5 milion Rs  Lolywood/Bolywood\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.137435913085938, 'negative': -11.223771095275879, 'neutral': -11.49982738494873}\n",
      "Tweet Prompt: \"This Saturday &amp; Sunday come join us the @user at the Pomona Fairplex! Your ticket can WIN you a Brand New Car! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.966142654418945, 'negative': -12.162409782409668, 'neutral': -12.199027061462402}\n",
      "Tweet Prompt: \"\"I do worry about the mentality of the average tweeter when I see Jeremy Kyle\\u002c and \\\"\"\"\"Christmas\\\"\"\"\" trending on November 1st...\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -11.441130638122559, 'negative': -12.456974029541016, 'neutral': -13.474821090698242}\n",
      "Tweet Prompt: \"@user we want you to milan tomorrow !!! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.19980525970459, 'negative': -12.044758796691895, 'neutral': -12.174792289733887}\n",
      "Tweet Prompt: \"Sharknado 3 may be the best film I've seen yet. #Sharknado3 #America \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -11.796079635620117, 'negative': -13.29868221282959, 'neutral': -14.582357406616211}\n",
      "Tweet Prompt: \"Celebrity Big Brother: Daniel's eviction stirs up bad feelings in the house: Daniel Baldwin may have left the ... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.59022045135498, 'negative': -12.6358003616333, 'neutral': -13.21710205078125}\n",
      "Tweet Prompt: \"Hey David Bowie Do u want to get iPh0ne 6 for FREE? U better check my bi0. Thx \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.563201904296875, 'negative': -11.216692924499512, 'neutral': -11.269148826599121}\n",
      "Tweet Prompt: \"@user @user I think that was faster win then the rousey fight on Saturday Night! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -11.416204452514648, 'negative': -12.792778015136719, 'neutral': -14.35031509399414}\n",
      "Tweet Prompt: \"\"Gay marriage is NOT a constitutional right! Not recognized with Christians, Buddists, or Islam for that matter.  \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -8.913990020751953, 'negative': -11.204320907592773, 'neutral': -12.487494468688965}\n",
      "Tweet Prompt: \"\"George Harrison's review of the Sun: \"\"It's all right.\"\"\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.67070198059082, 'negative': -12.336633682250977, 'neutral': -12.663342475891113}\n",
      "Tweet Prompt: \"\"this adorable old couple in dunkin literally made my day, he's turning 89 tomorrow and talked to me about how he was drafted for the WWII\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -8.82693862915039, 'negative': -11.452642440795898, 'neutral': -11.683629989624023}\n",
      "Tweet Prompt: \"@user @user Yellow journalism.  But you know?  This may be Harper's Waterloo \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -7.852314472198486, 'negative': -9.847879409790039, 'neutral': -10.034587860107422}\n",
      "Tweet Prompt: \"Chelsea Clinton is asked about Kanye West's run for president and her answer may surprise you: via @user NEVER!!! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.497871398925781, 'negative': -11.89190673828125, 'neutral': -12.618698120117188}\n",
      "Tweet Prompt: \"Monday at Town Ballroom: RICHIE HAWTIN with LOCO DICE.  Dude is so awesome.  Tix still avail at  \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -7.235465049743652, 'negative': -9.278902053833008, 'neutral': -8.803499221801758}\n",
      "Tweet Prompt: \"Evgeni Malkin doesn\\u2019t play again until the 16th. This NHL lockout sucks\\u002c I can\\u2019t watch games that are being played in Russia. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -11.554384231567383, 'negative': -13.051700592041016, 'neutral': -13.703460693359375}\n",
      "Tweet Prompt: \"Bowling tomorrow c; Don\\u2019t want things to be awkard lol \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.325478553771973, 'negative': -11.102997779846191, 'neutral': -10.86850643157959}\n",
      "Tweet Prompt: \"@user Happy 2nd Birthday to Prince George! Hope he has a fantastic day! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.879215240478516, 'negative': -12.147798538208008, 'neutral': -12.226419448852539}\n",
      "Tweet Prompt: \"\"Seriously, the SAT is just the TAKS on steroids .___.\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.29531192779541, 'negative': -11.69618034362793, 'neutral': -12.512330055236816}\n",
      "Tweet Prompt: \"Murray and Anderson have had 3 hour and 16 minute match and they are only going to the 4th set... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.493952751159668, 'negative': -11.24820327758789, 'neutral': -11.82890796661377}\n",
      "Tweet Prompt: \"\"According to Janet Jackson's long time producer Terry Lewis, the album is due in October.  STAY CONNECTED!... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -10.682888984680176, 'negative': -12.191495895385742, 'neutral': -12.849124908447266}\n",
      "Tweet Prompt: \"David Cameron's statement on camera on Thursday 03 September 2015: he will  take in 'more' of the refugees: was he speaking TO TV Cameras? \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.26442813873291, 'negative': -11.498180389404297, 'neutral': -11.515802383422852}\n",
      "Tweet Prompt: \"\"Few people remember or ever knew that in his rookie season, Tom Brady, in the Pats' pecking order of quarterbacks on the team, was 4th. 4TH!\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.708277702331543, 'negative': -11.286328315734863, 'neutral': -11.849777221679688}\n",
      "Tweet Prompt: \"@user have a fun trip tomorrow! Your note is on Tumblr is you didn\\u2019t see it yet :) I believe in you\\u002c I love you\\u002c and run fast <3 \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.696524620056152, 'negative': -11.730257034301758, 'neutral': -11.597346305847168}\n",
      "Tweet Prompt: \"@user I'm from Halifax, Nova Scotia. Not sure if we will go on March break, I'll have to see as it gets closer. Why?\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.373453140258789, 'negative': -11.903085708618164, 'neutral': -12.552193641662598}\n",
      "Tweet Prompt: \"Today's pod @user talks about the @user deal w/ the Patriots &amp; gives thoughts on NFL &amp; CFB for Thurs night \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.731740951538086, 'negative': -12.170578002929688, 'neutral': -12.35264778137207}\n",
      "Tweet Prompt: \"Very proud of Katy Perry in the breeders crown today ... 4th from an awkward draw .. Great drive Amanda .. First Aussie past the post .. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -12.040704727172852, 'negative': -13.796592712402344, 'neutral': -14.15509033203125}\n",
      "Tweet Prompt: \"GOP has set the bar very low with a 4th cycle promoting a grossly UN-AMERICAN idea: POTUS as elected autocrat. #Civics101 \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -11.974333763122559, 'negative': -13.542832374572754, 'neutral': -14.635611534118652}\n",
      "Tweet Prompt: \"\"Holly Holm is forgiven. Dana White and the Fertittas, pictured right now... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.257633209228516, 'negative': -11.41625690460205, 'neutral': -12.258211135864258}\n",
      "Tweet Prompt: \"This is going to be a fun 5 years. The makings of the next Prime Minister... George Osborne.... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -10.298514366149902, 'negative': -12.290127754211426, 'neutral': -12.519464492797852}\n",
      "Tweet Prompt: \"Friday night an it\\u2019s dead. Suppose I should go bed\\u002c watch spartacus and nurse a glass of rum as I have no coke:( #SendMeToAsleep \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.451009750366211, 'negative': -12.463275909423828, 'neutral': -12.97972297668457}\n",
      "Tweet Prompt: \"Free this week: August 3 Figure Four Weekly: Detailed look at why Gawker thinks the FBI covered for Hulk Hogan #wwe \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.140728950500488, 'negative': -11.580832481384277, 'neutral': -12.283418655395508}\n",
      "Tweet Prompt: \"@user  Hello Marie\\u002c Are you stay in Bkk?. May God bless you:-) \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -8.663375854492188, 'negative': -11.14254379272461, 'neutral': -9.916866302490234}\n",
      "Tweet Prompt: \"I'm gonna watch Sharknado 3 cause I have no tv shows to watch on a Wednesday not cause I enjoy it. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.501970291137695, 'negative': -12.174442291259766, 'neutral': -12.844619750976562}\n",
      "Tweet Prompt: \"Brooklyn Nets vs. the New York Knicks has been re-scheduled for November 26\\u002c nationally televised on TNT. Tip-off is scheduled for 7 pm. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -11.328025817871094, 'negative': -12.782831192016602, 'neutral': -13.082755088806152}\n",
      "Tweet Prompt: \"Listening to David Bowie's #Fashion to get in the mood for @user  Can't wait for Sat night \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.326654434204102, 'negative': -11.166238784790039, 'neutral': -10.676398277282715}\n",
      "Tweet Prompt: \"@user tom Brady did not deflate balls, but was suspended for 4 games bc he may or may not have known it was being done\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.923158645629883, 'negative': -11.6476469039917, 'neutral': -12.616832733154297}\n",
      "Tweet Prompt: \"@user Do you think David Wright will be in the starting lineup on Monday? Thinking of road tripping it to Philly! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.068042755126953, 'negative': -11.834721565246582, 'neutral': -12.078317642211914}\n",
      "Tweet Prompt: \"Omigod you guys I get to see HOLY MOTORS tomorrow. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -10.626140594482422, 'negative': -12.334595680236816, 'neutral': -13.104168891906738}\n",
      "Tweet Prompt: \"Fox Chicago\\u002c you make me angry. Playing the Vikings vs Redskins over the NFC Championship rematch 49ers vs Giants tomorrow???? \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.172344207763672, 'negative': -11.84691333770752, 'neutral': -12.197107315063477}\n",
      "Tweet Prompt: \"Hillary urges help for Syrian refugees: &lt;p&gt;Hillary Clinton called on the United Nations Wednesday to press cou... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -11.069520950317383, 'negative': -12.929300308227539, 'neutral': -12.494561195373535}\n",
      "Tweet Prompt: \"Sunday and a lovely #RaggedTalk on Skiing in Scotland by the freewheelin' Patrick D Whelan. 12:45OutoftheBlue&DrillHall #skiing #newnickname \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.733257293701172, 'negative': -11.989971160888672, 'neutral': -12.699227333068848}\n",
      "Tweet Prompt: \"REALLY??? why couldn't you have them open Pitt, MetLife, or Boston???? we're fighting \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.824638366699219, 'negative': -12.174431800842285, 'neutral': -12.589771270751953}\n",
      "Tweet Prompt: \"I'm at work for 14.5 hours tomorrow lmao someone make me go to bed and stop reading articles about nicki \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.668404579162598, 'negative': -11.499938011169434, 'neutral': -11.434412956237793}\n",
      "Tweet Prompt: \"@user fighters going to be a great day at Murrayfield please say happy 21st birthday to Kane that would top off his day \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.537830352783203, 'negative': -11.506522178649902, 'neutral': -12.439493179321289}\n",
      "Tweet Prompt: \"Btw fuck Durant for going to the OKlahoma game Saturday!! You went to Texas!!! #LonghornForLife \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.875638961791992, 'negative': -11.915447235107422, 'neutral': -13.041129112243652}\n",
      "Tweet Prompt: \"15th seed @user defeats 20th seed Thiem and he's through to the 4th round of the @user to play Bellucci or Murray! #USOpen \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.498956680297852, 'negative': -12.320174217224121, 'neutral': -13.305731773376465}\n",
      "Tweet Prompt: \"But it's a three day weekend and we see Ed Sheeran tomorrow (!!!!!) so things miiiight be looking up. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.925129890441895, 'negative': -11.628569602966309, 'neutral': -12.127354621887207}\n",
      "Tweet Prompt: \"@user I don't understand what the fuck's wrong with you! You may hate Green Day but don't bother the Green Day Family with that! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.178516387939453, 'negative': -11.749402046203613, 'neutral': -12.26471996307373}\n",
      "Tweet Prompt: \"Gucci gob ego ideal mystery may bare the two-sided your nose way composition against well-inclined kindled: R... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -11.167781829833984, 'negative': -13.028913497924805, 'neutral': -13.647855758666992}\n",
      "Tweet Prompt: \"Jay-Z sat in that Interview like a God showing that he was truly ahead of his time while the other niggas flirting with Foxy Brown \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -10.005501747131348, 'negative': -11.743839263916016, 'neutral': -12.074233055114746}\n",
      "Tweet Prompt: \"\"George Lincoln Rockwell was one of the 1st to recognize that Conservatives like @user Buckley, Goldwater &amp; Reagan were #Cucks for Israel.\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.723861694335938, 'negative': -11.492593765258789, 'neutral': -12.197493553161621}\n",
      "Tweet Prompt: \"@user #SportsHalloweenCostume What about Tony Romo on sunday\\u2019s game against the Giants?! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.300848007202148, 'negative': -12.278929710388184, 'neutral': -13.102363586425781}\n",
      "Tweet Prompt: \"When I wake up tomorrow I'll be in a different country. Whoa! I didn't run into a David Beckham at the airport. That's a bummer. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.705084800720215, 'negative': -11.667205810546875, 'neutral': -12.016051292419434}\n",
      "Tweet Prompt: \"Amazon Prime Day beats Black Friday says retailer Amazon Prime Day may have been an excuse for the retail... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -11.07354736328125, 'negative': -12.13006591796875, 'neutral': -12.27239990234375}\n",
      "Tweet Prompt: \"@user they go to a different place for brunch every Saturday. They started the tradition with IHOP \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.330503463745117, 'negative': -11.865106582641602, 'neutral': -12.097455024719238}\n",
      "Tweet Prompt: \"CINCH YOUR SADDLE is live on Amazon!  Only 99 cents until tomorrow evening.Thank you gift! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.685066223144531, 'negative': -11.500703811645508, 'neutral': -11.669013977050781}\n",
      "Tweet Prompt: \"Very bad idea 2 allow the dead body of Yakub to come to Mumbai-situation may become evn more tense @user @user @user \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.219134330749512, 'negative': -10.746464729309082, 'neutral': -11.272157669067383}\n",
      "Tweet Prompt: \"\"Before we were all Charlie Hebdo, now we are all Parisian - tomorrow we may have to identify with others, but we must not be intimidated\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.762557983398438, 'negative': -12.354418754577637, 'neutral': -12.344860076904297}\n",
      "Tweet Prompt: \"\"The sun shall not smite I by day, nor the moon by night\" Bob Marley &amp; the Wailers NIGHT SHIFT *Live in NYC 4/30/76* \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.807147979736328, 'negative': -11.342290878295898, 'neutral': -11.153682708740234}\n",
      "Tweet Prompt: \"The graves are full with people who thought they would practise islam when older not even everyone is promised tomorrow #Realtalk \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.186594009399414, 'negative': -12.223389625549316, 'neutral': -12.85342025756836}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m prompt_ids \u001b[38;5;241m=\u001b[39m llama_3_2_1B_tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllama_3_2_1B_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# get the output logits\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encode possible labels \n",
    "sentiment_words = [\"positive\", \"negative\", \"neutral\"]\n",
    "sentiment_ids = [ids[0] for ids in llama_3_2_1B_tokenizer(sentiment_words, add_special_tokens=False).input_ids]\n",
    "\n",
    "\n",
    "\n",
    "# # iterate through the validation set\n",
    "for tweet, label in zip(ds_validation['text'], ds_validation['label']):\n",
    "    prompt_tweet = tweet\n",
    "    gt_label = label\n",
    "    # combine the prompt\n",
    "    prompt = prompt_template.format(prompt_tweet)\n",
    "    \n",
    "    prompt_ids = llama_3_2_1B_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llama_3_2_1B_model(prompt_ids)\n",
    "    \n",
    "        # get the output logits\n",
    "        logits = outputs.logits\n",
    "\n",
    "    \n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # build a list of the sentiment options and the probability per sentiment\n",
    "    tweet_sentiment_probabilities = {}\n",
    "\n",
    "    for sentiment, sentiment_id in zip(sentiment_words, sentiment_ids):\n",
    "            tweet_sentiment_probabilities[sentiment] = log_probs[0, -1, sentiment_id].item()\n",
    "    print(f\"Tweet Prompt: {prompt}\")\n",
    "    print(f\"Ground Truth: {gt_label}\")\n",
    "    print(tweet_sentiment_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0633f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Prompt: \"@user @user I think after Charlie Hebdo the French did NOT react as the US did after 9/11. But they may do this time around. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.349617004394531, 'negative': -10.979355812072754, 'neutral': -10.924836158752441}\n",
      "Tweet Prompt: \"\"Interview with Devon Alexander \\\"\"\"\"Speed Kills\\\"\"\"\" (VIDEO)  On Tuesday Oct 16th we had the privilege of catch up with... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.161550521850586, 'negative': -11.263745307922363, 'neutral': -10.887892723083496}\n",
      "Tweet Prompt: \"Hold on... Sam Smith may do the theme to Spectre!? Dope!!!!!! #007 #SPECTRE #JamesBond \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.673638343811035, 'negative': -11.593548774719238, 'neutral': -11.763683319091797}\n",
      "Tweet Prompt: \"kingpin Saudi Arabia posted a record $98 billion budget deficit in 2015 due to the sharp fall in oil prices finance ministry said on Monday \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.686995506286621, 'negative': -12.42932415008545, 'neutral': -11.51211166381836}\n",
      "Tweet Prompt: \"Gonna watch Final Destination 5 tonight. I always leave the theater so afraid of everything. No huge escalators for sure :S \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.941680908203125, 'negative': -12.832448959350586, 'neutral': -12.955827713012695}\n",
      "Tweet Prompt: \"Beautiful Bouquet with our Beautiful Bentley #bride #groom #wedding #wednesday #weddingcars #love   #Repost... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.185047149658203, 'negative': -10.976482391357422, 'neutral': -10.538427352905273}\n",
      "Tweet Prompt: \"@user @user Islam is an Abrahamic faith, Andrew. It may make you feel a little uneasy but it's the same God you worship. Sorry.\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.06990909576416, 'negative': -11.328580856323242, 'neutral': -11.701764106750488}\n",
      "Tweet Prompt: \"Harper's Worst Offense against Refugees may be Climate Record as rising temperatures add to chaos in the Middle East \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -11.200645446777344, 'negative': -12.547121047973633, 'neutral': -13.033374786376953}\n",
      "Tweet Prompt: \"@user call Hafiz saeed sir he may help u out. Maybe Pope can b handy . Try it. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -8.83209228515625, 'negative': -10.957419395446777, 'neutral': -10.792651176452637}\n",
      "Tweet Prompt: \"Disappointed the Knicks vs Nets game got canceled tonight\\u002c but I\\u2019m even more hyped for Knicks vs Heat on Friday! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -11.655611038208008, 'negative': -12.956754684448242, 'neutral': -13.735713958740234}\n",
      "Tweet Prompt: \"\"LONDON (AP) \"\" Prince George celebrates his second birthday on Wednesday and while he's just a toddler, he's al...  \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.640953063964844, 'negative': -11.291556358337402, 'neutral': -10.707857131958008}\n",
      "Tweet Prompt: \"\"\\\"\"\"\"@_eryflores: March 16 Luke Bryan is gonna at the Houston Rodeo. I HAVE to go\\u002c Its a MUST!\\\"\"\"\"\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.344711303710938, 'negative': -11.380562782287598, 'neutral': -11.035704612731934}\n",
      "Tweet Prompt: \"It is reality that ISIS are on the march in Turkey and Erdogan can't wait to receive them with open arms \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -11.595776557922363, 'negative': -13.202943801879883, 'neutral': -13.172237396240234}\n",
      "Tweet Prompt: \"@user  A trunk show by Pipa & bella & EKSMS on Nov 1st @ Escobar with complimentary Cocktail workshop & designer Jewelry.RSVP to us \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.780923843383789, 'negative': -11.680092811584473, 'neutral': -10.716678619384766}\n",
      "Tweet Prompt: \"PM ready for reply on coal blocks: Congress: New Delhi\\u002c Aug 22 (IANS) With the Bharatiya Janata Party (BJP)... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -11.572053909301758, 'negative': -13.475152015686035, 'neutral': -12.86563491821289}\n",
      "Tweet Prompt: \"\"More like boring eagles\"\"\"\"\"\"\"\"@Tunnyking: C'mon bro, Go out and support the Super Eagles #RT @user I hate international breaks\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.810250282287598, 'negative': -12.360438346862793, 'neutral': -13.178675651550293}\n",
      "Tweet Prompt: \"\"The BAGRANGI new Pic,Of SALMAN khan That VERY FAMOUS IN PAK CENEMA'S at the 1st day of EID that pic,made 1.5 milion Rs  Lolywood/Bolywood\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.137435913085938, 'negative': -11.223771095275879, 'neutral': -11.49982738494873}\n",
      "Tweet Prompt: \"This Saturday &amp; Sunday come join us the @user at the Pomona Fairplex! Your ticket can WIN you a Brand New Car! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.966142654418945, 'negative': -12.162409782409668, 'neutral': -12.199027061462402}\n",
      "Tweet Prompt: \"\"I do worry about the mentality of the average tweeter when I see Jeremy Kyle\\u002c and \\\"\"\"\"Christmas\\\"\"\"\" trending on November 1st...\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -11.441130638122559, 'negative': -12.456974029541016, 'neutral': -13.474821090698242}\n",
      "Tweet Prompt: \"@user we want you to milan tomorrow !!! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.19980525970459, 'negative': -12.044758796691895, 'neutral': -12.174792289733887}\n",
      "Tweet Prompt: \"Sharknado 3 may be the best film I've seen yet. #Sharknado3 #America \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -11.796079635620117, 'negative': -13.29868221282959, 'neutral': -14.582357406616211}\n",
      "Tweet Prompt: \"Celebrity Big Brother: Daniel's eviction stirs up bad feelings in the house: Daniel Baldwin may have left the ... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.59022045135498, 'negative': -12.6358003616333, 'neutral': -13.21710205078125}\n",
      "Tweet Prompt: \"Hey David Bowie Do u want to get iPh0ne 6 for FREE? U better check my bi0. Thx \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.563201904296875, 'negative': -11.216692924499512, 'neutral': -11.269148826599121}\n",
      "Tweet Prompt: \"@user @user I think that was faster win then the rousey fight on Saturday Night! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -11.416204452514648, 'negative': -12.792778015136719, 'neutral': -14.35031509399414}\n",
      "Tweet Prompt: \"\"Gay marriage is NOT a constitutional right! Not recognized with Christians, Buddists, or Islam for that matter.  \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -8.913990020751953, 'negative': -11.204320907592773, 'neutral': -12.487494468688965}\n",
      "Tweet Prompt: \"\"George Harrison's review of the Sun: \"\"It's all right.\"\"\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.67070198059082, 'negative': -12.336633682250977, 'neutral': -12.663342475891113}\n",
      "Tweet Prompt: \"\"this adorable old couple in dunkin literally made my day, he's turning 89 tomorrow and talked to me about how he was drafted for the WWII\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -8.82693862915039, 'negative': -11.452642440795898, 'neutral': -11.683629989624023}\n",
      "Tweet Prompt: \"@user @user Yellow journalism.  But you know?  This may be Harper's Waterloo \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -7.852314472198486, 'negative': -9.847879409790039, 'neutral': -10.034587860107422}\n",
      "Tweet Prompt: \"Chelsea Clinton is asked about Kanye West's run for president and her answer may surprise you: via @user NEVER!!! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.497871398925781, 'negative': -11.89190673828125, 'neutral': -12.618698120117188}\n",
      "Tweet Prompt: \"Monday at Town Ballroom: RICHIE HAWTIN with LOCO DICE.  Dude is so awesome.  Tix still avail at  \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -7.235465049743652, 'negative': -9.278902053833008, 'neutral': -8.803499221801758}\n",
      "Tweet Prompt: \"Evgeni Malkin doesn\\u2019t play again until the 16th. This NHL lockout sucks\\u002c I can\\u2019t watch games that are being played in Russia. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -11.554384231567383, 'negative': -13.051700592041016, 'neutral': -13.703460693359375}\n",
      "Tweet Prompt: \"Bowling tomorrow c; Don\\u2019t want things to be awkard lol \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.325478553771973, 'negative': -11.102997779846191, 'neutral': -10.86850643157959}\n",
      "Tweet Prompt: \"@user Happy 2nd Birthday to Prince George! Hope he has a fantastic day! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.879215240478516, 'negative': -12.147798538208008, 'neutral': -12.226419448852539}\n",
      "Tweet Prompt: \"\"Seriously, the SAT is just the TAKS on steroids .___.\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.29531192779541, 'negative': -11.69618034362793, 'neutral': -12.512330055236816}\n",
      "Tweet Prompt: \"Murray and Anderson have had 3 hour and 16 minute match and they are only going to the 4th set... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.493952751159668, 'negative': -11.24820327758789, 'neutral': -11.82890796661377}\n",
      "Tweet Prompt: \"\"According to Janet Jackson's long time producer Terry Lewis, the album is due in October.  STAY CONNECTED!... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -10.682888984680176, 'negative': -12.191495895385742, 'neutral': -12.849124908447266}\n",
      "Tweet Prompt: \"David Cameron's statement on camera on Thursday 03 September 2015: he will  take in 'more' of the refugees: was he speaking TO TV Cameras? \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.26442813873291, 'negative': -11.498180389404297, 'neutral': -11.515802383422852}\n",
      "Tweet Prompt: \"\"Few people remember or ever knew that in his rookie season, Tom Brady, in the Pats' pecking order of quarterbacks on the team, was 4th. 4TH!\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.708277702331543, 'negative': -11.286328315734863, 'neutral': -11.849777221679688}\n",
      "Tweet Prompt: \"@user have a fun trip tomorrow! Your note is on Tumblr is you didn\\u2019t see it yet :) I believe in you\\u002c I love you\\u002c and run fast <3 \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.696524620056152, 'negative': -11.730257034301758, 'neutral': -11.597346305847168}\n",
      "Tweet Prompt: \"@user I'm from Halifax, Nova Scotia. Not sure if we will go on March break, I'll have to see as it gets closer. Why?\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.373453140258789, 'negative': -11.903085708618164, 'neutral': -12.552193641662598}\n",
      "Tweet Prompt: \"Today's pod @user talks about the @user deal w/ the Patriots &amp; gives thoughts on NFL &amp; CFB for Thurs night \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.731740951538086, 'negative': -12.170578002929688, 'neutral': -12.35264778137207}\n",
      "Tweet Prompt: \"Very proud of Katy Perry in the breeders crown today ... 4th from an awkward draw .. Great drive Amanda .. First Aussie past the post .. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -12.040704727172852, 'negative': -13.796592712402344, 'neutral': -14.15509033203125}\n",
      "Tweet Prompt: \"GOP has set the bar very low with a 4th cycle promoting a grossly UN-AMERICAN idea: POTUS as elected autocrat. #Civics101 \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -11.974333763122559, 'negative': -13.542832374572754, 'neutral': -14.635611534118652}\n",
      "Tweet Prompt: \"\"Holly Holm is forgiven. Dana White and the Fertittas, pictured right now... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.257633209228516, 'negative': -11.41625690460205, 'neutral': -12.258211135864258}\n",
      "Tweet Prompt: \"This is going to be a fun 5 years. The makings of the next Prime Minister... George Osborne.... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -10.298514366149902, 'negative': -12.290127754211426, 'neutral': -12.519464492797852}\n",
      "Tweet Prompt: \"Friday night an it\\u2019s dead. Suppose I should go bed\\u002c watch spartacus and nurse a glass of rum as I have no coke:( #SendMeToAsleep \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.451009750366211, 'negative': -12.463275909423828, 'neutral': -12.97972297668457}\n",
      "Tweet Prompt: \"Free this week: August 3 Figure Four Weekly: Detailed look at why Gawker thinks the FBI covered for Hulk Hogan #wwe \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.140728950500488, 'negative': -11.580832481384277, 'neutral': -12.283418655395508}\n",
      "Tweet Prompt: \"@user  Hello Marie\\u002c Are you stay in Bkk?. May God bless you:-) \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -8.663375854492188, 'negative': -11.14254379272461, 'neutral': -9.916866302490234}\n",
      "Tweet Prompt: \"I'm gonna watch Sharknado 3 cause I have no tv shows to watch on a Wednesday not cause I enjoy it. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.501970291137695, 'negative': -12.174442291259766, 'neutral': -12.844619750976562}\n",
      "Tweet Prompt: \"Brooklyn Nets vs. the New York Knicks has been re-scheduled for November 26\\u002c nationally televised on TNT. Tip-off is scheduled for 7 pm. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -11.328025817871094, 'negative': -12.782831192016602, 'neutral': -13.082755088806152}\n",
      "Tweet Prompt: \"Listening to David Bowie's #Fashion to get in the mood for @user  Can't wait for Sat night \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.326654434204102, 'negative': -11.166238784790039, 'neutral': -10.676398277282715}\n",
      "Tweet Prompt: \"@user tom Brady did not deflate balls, but was suspended for 4 games bc he may or may not have known it was being done\" \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.923158645629883, 'negative': -11.6476469039917, 'neutral': -12.616832733154297}\n",
      "Tweet Prompt: \"@user Do you think David Wright will be in the starting lineup on Monday? Thinking of road tripping it to Philly! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.068042755126953, 'negative': -11.834721565246582, 'neutral': -12.078317642211914}\n",
      "Tweet Prompt: \"Omigod you guys I get to see HOLY MOTORS tomorrow. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -10.626140594482422, 'negative': -12.334595680236816, 'neutral': -13.104168891906738}\n",
      "Tweet Prompt: \"Fox Chicago\\u002c you make me angry. Playing the Vikings vs Redskins over the NFC Championship rematch 49ers vs Giants tomorrow???? \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.172344207763672, 'negative': -11.84691333770752, 'neutral': -12.197107315063477}\n",
      "Tweet Prompt: \"Hillary urges help for Syrian refugees: &lt;p&gt;Hillary Clinton called on the United Nations Wednesday to press cou... \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -11.069520950317383, 'negative': -12.929300308227539, 'neutral': -12.494561195373535}\n",
      "Tweet Prompt: \"Sunday and a lovely #RaggedTalk on Skiing in Scotland by the freewheelin' Patrick D Whelan. 12:45OutoftheBlue&DrillHall #skiing #newnickname \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.733257293701172, 'negative': -11.989971160888672, 'neutral': -12.699227333068848}\n",
      "Tweet Prompt: \"REALLY??? why couldn't you have them open Pitt, MetLife, or Boston???? we're fighting \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.824638366699219, 'negative': -12.174431800842285, 'neutral': -12.589771270751953}\n",
      "Tweet Prompt: \"I'm at work for 14.5 hours tomorrow lmao someone make me go to bed and stop reading articles about nicki \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -9.668404579162598, 'negative': -11.499938011169434, 'neutral': -11.434412956237793}\n",
      "Tweet Prompt: \"@user fighters going to be a great day at Murrayfield please say happy 21st birthday to Kane that would top off his day \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.537830352783203, 'negative': -11.506522178649902, 'neutral': -12.439493179321289}\n",
      "Tweet Prompt: \"Btw fuck Durant for going to the OKlahoma game Saturday!! You went to Texas!!! #LonghornForLife \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -9.875638961791992, 'negative': -11.915447235107422, 'neutral': -13.041129112243652}\n",
      "Tweet Prompt: \"15th seed @user defeats 20th seed Thiem and he's through to the 4th round of the @user to play Bellucci or Murray! #USOpen \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 1\n",
      "{'positive': -10.498956680297852, 'negative': -12.320174217224121, 'neutral': -13.305731773376465}\n",
      "Tweet Prompt: \"But it's a three day weekend and we see Ed Sheeran tomorrow (!!!!!) so things miiiight be looking up. \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 2\n",
      "{'positive': -9.925129890441895, 'negative': -11.628569602966309, 'neutral': -12.127354621887207}\n",
      "Tweet Prompt: \"@user I don't understand what the fuck's wrong with you! You may hate Green Day but don't bother the Green Day Family with that! \" \n",
      " Sentiment (positive (2), negative (0), neutral (1)): \n",
      "Ground Truth: 0\n",
      "{'positive': -10.178516387939453, 'negative': -11.749402046203613, 'neutral': -12.26471996307373}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m prompt_ids \u001b[38;5;241m=\u001b[39m llama_3_2_1B_tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllama_3_2_1B_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# get the output logits\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Large-Language-Models/Assignments/LLM-PSU-Assignment-2/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# encode possible labels\n",
    "sentiment_words = [\"positive\", \"negative\", \"neutral\"]\n",
    "sentiment_ids = [ids[0] for ids in llama_3_2_1B_tokenizer(sentiment_words, add_special_tokens=False).input_ids]\n",
    "\n",
    "\n",
    "# # iterate through the validation set\n",
    "for tweet, label in zip(ds_validation['text'], ds_validation['label']):\n",
    "    prompt_tweet = tweet\n",
    "    gt_label = label\n",
    "    # combine the prompt\n",
    "    prompt = prompt_template.format(prompt_tweet)\n",
    "\n",
    "    prompt_ids = llama_3_2_1B_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llama_3_2_1B_model(prompt_ids)\n",
    "\n",
    "        # get the output logits\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # build a list of the sentiment options and the probability per sentiment\n",
    "    tweet_sentiment_probabilities = {}\n",
    "\n",
    "    for sentiment, sentiment_id in zip(sentiment_words, sentiment_ids):\n",
    "        tweet_sentiment_probabilities[sentiment] = log_probs[0, -1, sentiment_id].item()\n",
    "    print(f\"Tweet Prompt: {prompt}\")\n",
    "    print(f\"Ground Truth: {gt_label}\")\n",
    "    print(tweet_sentiment_probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
